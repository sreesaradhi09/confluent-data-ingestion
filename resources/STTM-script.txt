Demo Script:
===========

ğŸ™ï¸ Script for Explaining the Latest STTM to Analysts
Opening (Context & Purpose)

â€œAlright team, letâ€™s go over the latest STTM version â€” this is version sixteen, and itâ€™s our standardized blueprint for defining how data flows from Kafka topics into our Flink tables.
The goal of this document is simple â€” instead of writing SQL scripts manually, we capture every transformation, join, and rule as structured metadata here.
The Python generator then reads this sheet and produces all the Flink DDL and DML automatically â€” from raw JSON to clean Iceberg tables.â€

Section 1 â€“ How the STTM is Structured
â€œIf you open the sheet named STTM_Mapping, youâ€™ll see that each row represents exactly one target column.â€¨That means for every column we want to build, we record where it comes from, how itâ€™s transformed, and where it lands.
The document is divided into three logical stages â€”â€¨1ï¸âƒ£ View â€“ where we parse JSON from Kafka into a table format,â€¨2ï¸âƒ£ XREF â€“ where we build snapshot or cross-reference tables using primary key upserts, andâ€¨3ï¸âƒ£ FGAC â€“ where we apply fine-grained access control logic and enrich or mask the final data.
Youâ€™ll always see the rows ordered in that sequence: Views first, then XREF, then FGAC.â€

Section 2 â€“ Key Columns and How to Fill Them
â€œLetâ€™s walk through the key columns youâ€™ll be working with.
	â€¢	PipelineStage â€“ choose one of View, XREF, or FGAC.
	â€¢	TargetTable â€“ the name of the table being built, like CBA_CI_view or XREF_CBA_CI.
	â€¢	TargetColumn â€“ the specific output column, like CI_ID, EMAIL_MASKED, or LINE1.
	â€¢	TargetDataType â€“ the final data type. Thereâ€™s a dropdown so you can select from allowed types â€” like STRING, INT, DATE, etc.
	â€¢	IsTargetPK â€“ mark â€˜Yâ€™ for primary key fields. These rows will be highlighted in green.
	â€¢	TargetPK â€“ you only fill this for key columns; for non-keys, leave it blank.
	â€¢	SourcePrimaryTable and SourceField â€“ define where the data comes from, whether itâ€™s a Kafka topic, another view, or an XREF table.
	â€¢	SourceTransformExpr â€“ this is where you describe how the field is derived.â€¨Keep it to expressions only â€” for example:â€¨CAST(JSON_VALUE(CAST(val AS STRING), '$.CI_ID') AS INT)â€¨No full SQL statements here.
	â€¢	FilterPredicate â€“ this goes only on the primary key row of a view. It tells Flink which records belong to that logical table, since a Kafka topic may carry multiple tables.â€¨For example: INFA_TABLE_NAME LIKE '%_CBA_CI'.
	â€¢	JoinTable1 / JoinAlias1 / JoinType1 / JoinCondition1 â€“ used when the target table is built from multiple inputs.â€¨You simply enter the other tableâ€™s name, alias, and join condition â€” for example:â€¨JoinType1 = LEFT and JoinCondition1 = TRIM(clh.CL_N) = TRIM(cov.CNTR_ID).â€¨Just the condition â€” donâ€™t include the keyword â€˜LEFT ONâ€™ or â€˜INNER ONâ€™.
	â€¢	Notes â€“ this is optional, but use it to clarify business rules or transformations, like â€˜Mask PIIâ€™ or â€˜Derived from parent snapshotâ€™.â€

Section 3 â€“ Configuration, Naming & Conventions
â€œNow, a few important conventions that make this document work seamlessly with the Flink generator.
All Flink tables follow a consistent naming pattern:
	â€¢	Source views end with _view
	â€¢	Snapshot tables begin with XREF_
	â€¢	FGAC outputs end with _REF or _REFINED.
The generator automatically applies standard Flink configurations based on these names â€”â€¨things like value.format = 'avro-registry', changelog.mode = 'upsert', and Kafka topic details â€” so you donâ€™t have to enter them manually.
As long as you follow the prefix and suffix conventions, the pipeline will build the configuration automatically.â€

Section 4 â€“ Best Practices and Quality Checks
â€œBefore you submit the STTM, review these few items:
	â€¢	Make sure all primary keys are marked correctly â€” green rows are your clue.
	â€¢	Confirm thereâ€™s only one filter per view.
	â€¢	Check that join conditions are clean and use aliases consistently.
	â€¢	Donâ€™t rename or delete columns â€” theyâ€™re required by the generator.
	â€¢	Finally, keep the order: Views, XREF, and FGAC â€” in that exact sequence.â€

Closing (Reinforcement & Value)
â€œOnce this document is complete, the automation takes over.â€¨The generator will validate syntax, build all Flink SQL files, and package them for deployment automatically.
So the accuracy and clarity of your mappings directly determine how cleanly the SQL gets generated.
If you stick to the structure â€” no manual SQL, correct stage order, and consistent naming â€” weâ€™ll get repeatable, traceable, and production-ready data flows with almost zero manual coding.
Thatâ€™s the power of this STTM â€” itâ€™s not just a spreadsheet; itâ€™s the blueprint for the entire ingestion pipeline.â€

=============================================================================================================

Build an STTM File in excel format

â€œWhen we build an STTM file, weâ€™re basically creating a blueprint for how data moves and transforms â€” from the raw Kafka topic all the way to the final tables the business will use.

We start by identifying where the data comes from â€” in this case, the master Kafka topic that carries consumer information.Next, we define what each field means, how itâ€™s formatted, and how we want to use it. Since the Kafka message is in JSON, we first convert it into a Flink view, which is like a clean, readable table version of that message.

Once the data is structured in views, we decide how to build the XREF tables. These tables combine and clean up the data â€” removing duplicates and keeping one accurate record per consumer. Here, we also define the business keys, such as customer ID or email, that help identify a unique record.

After that, we build the FGAC tables (Fine-Grained Access Control). These are built by joining the XREF table with child or entitlement tables, so that only authorized users can see certain rows or columns â€” for example, one region might only see its own customers.

In the STTM file, each sheet or section describes:
	â€¢	What the source data is and how it looks,
	â€¢	The logic or rules for cleaning and transforming it,
	â€¢	The target table where it lands,
	â€¢	And any filters, joins, or special conditions we apply.

So in short â€” the STTM tells the full story of how raw data becomes trusted, secure, business-ready data. Itâ€™s what the developers use to write SQL, and what the business uses to verify the logic and outputs.â€


Here are the requirements or you.

Source data is ingested into Kafka topic in JSON string format. The kafka topic is a schema level topic which means it contains the JSON messages for multiple tables.

Create an STTM file which servers a blueprint for how the data is transformed from Kafka topis to data lake tables in iceberg format using Flink SQL.

First, consume the events from kafka topic and create view in Flink. These views contains the logic to convert JSON string into table format using built-in json functions in flink. The data type conversion from sting to appropriate data type should also take place in view.

Next, build snapshpot tables called as xref using prinary key upsert logic in flink sql. These tables are created from from the views developed earlier.

Next, the parent table is CBA_CI. The snapshot of this table is joined with address table to generate the refined address table with FGAC columns.

The STTM should also have feature for expressions, join conditions, primary keys, and other required constructs required for this use case and flink sql.

I want you to generate an STTM file for the above mentioned use case.


